# Respostas


# Tente valores diferentes do argumento num_examples na função load_data_nmt. Como isso afeta os tamanhos do vocabulário do idioma de origem e do idioma de destino?


À medida que aumentamos o valor de `num_examples`, os tamanhos dos vocabulários em ambos os idiomas crescem significativamente. Com apenas 100 exemplos, o vocabulário em inglês contém 40 palavras, enquanto o francês tem 39. Utilizando todos os exemplos disponíveis (num_examples=None), os vocabulários expandem para 10.012 palavras em inglês e 17.851 em francês. Cada novo exemplo pode introduzir palavras únicas que não estavam presentes anteriormente, e um conjunto de dados maior abrange uma variedade maior de expressões, contextos e terminologias, resultando em um vocabulário mais extenso. Palavras comuns aparecerão repetidamente, mas as menos frequentes só surgirão ao incluir mais exemplos, o que explica por que o crescimento do vocabulário não é linear; inicialmente, há um rápido aumento que tende a desacelerar à medida que mais exemplos são adicionados. O parâmetro `num_examples` controla a quantidade de dados utilizados para construir os vocabulários, e um valor maior leva a um vocabulário mais abrangente, capturando mais palavras e expressões do idioma. Contudo, um vocabulário maior pode melhorar a capacidade do modelo de lidar com diversidade linguística, mas também pode aumentar a complexidade computacional. Portanto, é importante balancear o tamanho do vocabulário para garantir que o modelo seja eficiente e eficaz.


# O texto em alguns idiomas, como chinês e japonês, não tem indicadores de limite de palavras (por exemplo, espaço). A tokenização em nível de palavra ainda é uma boa ideia para esses casos? Por que ou por que não?


Ao analisar os resultados, observamos que o texto pré-processado em chinês permanece contínuo, sem espaços entre caracteres ou palavras, enquanto o texto em inglês é convertido para minúsculas e os sinais de pontuação são separados por espaços. Na tokenização, a função `split(' ')` não separa o texto chinês, pois não há espaços para usar como delimitadores, resultando em cada linha sendo tratada como um único token, como `[['我爱你。'], ['你好吗？']]`. Em contraste, o texto em inglês é corretamente tokenizado em palavras individuais. Isso evidencia as limitações da tokenização baseada em espaços para idiomas como o chinês, onde as palavras não são separadas por espaços, fazendo com que a tokenização padrão falhe em segmentar o texto apropriadamente e resulte em tokens que são frases inteiras, dificultando a análise e o processamento do idioma. Portanto, é essencial utilizar métodos de segmentação de palavras projetados para lidar com a estrutura desses idiomas, como ferramentas específicas como o Jieba para chinês ou o MeCab para japonês, que podem segmentar o texto em palavras significativas. Concluímos que a tokenização em nível de palavra baseada em espaços não é eficaz para o chinês e o japonês devido à ausência de delimitadores explícitos. Abordagens alternativas incluem a tokenização em nível de caracteres, onde cada caractere é tratado como um token mas pode não capturar o significado completo de palavras compostas, a segmentação de palavras utilizando algoritmos que identificam fronteiras de palavras com base em modelos estatísticos ou dicionários, e modelos baseados em subword units, como técnicas de Byte Pair Encoding (BPE), que podem ser utilizadas para lidar com idiomas de morfologia complexa. Essas escolhas têm implicações práticas, pois a técnica de tokenização afeta diretamente o desempenho de modelos de NLP, e é fundamental adaptar as ferramentas de processamento às características específicas de cada idioma para obter resultados confiáveis.
